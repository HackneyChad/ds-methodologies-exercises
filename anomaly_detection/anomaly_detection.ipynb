{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## anomaly_detection\n",
    "for all exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete data + probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques for identifying/detecting anomalies\n",
    "#### Statistical Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linspace, loadtxt, ones, convolve\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from random import randint\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import collections\n",
    "\n",
    "# set default pandas decimal number display format and df display format:\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle Data\n",
    "\n",
    "#### Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['ip', 'timestamp', 'request_method', 'status', 'size',\n",
    "          'destination', 'request_agent']\n",
    "df_orig = pd.read_csv('http://python.zach.lol/access.log',          \n",
    "                 engine='python',\n",
    "                 header=None,\n",
    "                 index_col=False,\n",
    "                 names=colnames,\n",
    "                 sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])',\n",
    "                 na_values='\"-\"',\n",
    "                 usecols=[0, 3, 4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.DataFrame([[\"95.31.18.119\", \"[21/Apr/2019:10:02:41+0000]\", \n",
    "                     \"GET /api/v1/items/HTTP/1.1\", 200, 1153005, np.nan, \n",
    "                     \"python-requests/2.21.0\"],\n",
    "                    [\"95.31.16.121\", \"[17/Apr/2019:19:36:41+0000]\", \n",
    "                     \"GET /api/v1/sales?page=79/HTTP/1.1\", 301, 1005, np.nan, \n",
    "                     \"python-requests/2.21.0\"],\n",
    "                    [\"97.105.15.120\", \"[18/Apr/2019:19:42:41+0000]\", \n",
    "                     \"GET /api/v1/sales?page=79/HTTP/1.1\", 301, 2560, np.nan, \n",
    "                     \"python-requests/2.21.0\"],\n",
    "                    [\"97.105.19.58\", \"[19/Apr/2019:19:42:41+0000]\", \n",
    "                     \"GET /api/v1/sales?page=79/HTTP/1.1\", 200, 2056327, np.nan, \n",
    "                     \"python-requests/2.21.0\"]], columns=colnames)\n",
    "\n",
    "df = df_orig.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>request_method</th>\n",
       "      <th>status</th>\n",
       "      <th>size</th>\n",
       "      <th>destination</th>\n",
       "      <th>request_agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>[16/Apr/2019:19:34:42 +0000]</td>\n",
       "      <td>\"GET /api/v1/sales?page=81 HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>512495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>[16/Apr/2019:19:34:42 +0000]</td>\n",
       "      <td>\"GET /api/v1/items HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>3561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>[16/Apr/2019:19:34:44 +0000]</td>\n",
       "      <td>\"GET /api/v1/sales?page=82 HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>510103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>[16/Apr/2019:19:34:46 +0000]</td>\n",
       "      <td>\"GET /api/v1/sales?page=83 HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>510003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>[16/Apr/2019:19:34:48 +0000]</td>\n",
       "      <td>\"GET /api/v1/sales?page=84 HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>511963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ip                     timestamp                        request_method  status    size destination             request_agent\n",
       "0  97.105.19.58  [16/Apr/2019:19:34:42 +0000]  \"GET /api/v1/sales?page=81 HTTP/1.1\"     200  512495         NaN  \"python-requests/2.21.0\"\n",
       "1  97.105.19.58  [16/Apr/2019:19:34:42 +0000]          \"GET /api/v1/items HTTP/1.1\"     200    3561         NaN  \"python-requests/2.21.0\"\n",
       "2  97.105.19.58  [16/Apr/2019:19:34:44 +0000]  \"GET /api/v1/sales?page=82 HTTP/1.1\"     200  510103         NaN  \"python-requests/2.21.0\"\n",
       "3  97.105.19.58  [16/Apr/2019:19:34:46 +0000]  \"GET /api/v1/sales?page=83 HTTP/1.1\"     200  510003         NaN  \"python-requests/2.21.0\"\n",
       "4  97.105.19.58  [16/Apr/2019:19:34:48 +0000]  \"GET /api/v1/sales?page=84 HTTP/1.1\"     200  511963         NaN  \"python-requests/2.21.0\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13978 entries, 0 to 3\n",
      "Data columns (total 7 columns):\n",
      "ip                13978 non-null object\n",
      "timestamp         13978 non-null object\n",
      "request_method    13978 non-null object\n",
      "status            13978 non-null int64\n",
      "size              13978 non-null int64\n",
      "destination       25 non-null object\n",
      "request_agent     13978 non-null object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 873.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.timestamp = df.timestamp.str.replace(r'(\\[|\\])', '', regex=True)\n",
    "df.timestamp = pd.to_datetime(df.timestamp.str.replace(':', ' ', 1)) \n",
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>request_method</th>\n",
       "      <th>status</th>\n",
       "      <th>size</th>\n",
       "      <th>destination</th>\n",
       "      <th>request_agent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:42</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>\"GET /api/v1/sales?page=81 HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>512495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:42</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>\"GET /api/v1/items HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>3561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:44</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>\"GET /api/v1/sales?page=82 HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>510103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:46</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>\"GET /api/v1/sales?page=83 HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>510003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:48</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>\"GET /api/v1/sales?page=84 HTTP/1.1\"</td>\n",
       "      <td>200</td>\n",
       "      <td>511963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"python-requests/2.21.0\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ip                        request_method  status    size destination             request_agent\n",
       "timestamp                                                                                                                    \n",
       "2019-04-16 19:34:42  97.105.19.58  \"GET /api/v1/sales?page=81 HTTP/1.1\"     200  512495         NaN  \"python-requests/2.21.0\"\n",
       "2019-04-16 19:34:42  97.105.19.58          \"GET /api/v1/items HTTP/1.1\"     200    3561         NaN  \"python-requests/2.21.0\"\n",
       "2019-04-16 19:34:44  97.105.19.58  \"GET /api/v1/sales?page=82 HTTP/1.1\"     200  510103         NaN  \"python-requests/2.21.0\"\n",
       "2019-04-16 19:34:46  97.105.19.58  \"GET /api/v1/sales?page=83 HTTP/1.1\"     200  510003         NaN  \"python-requests/2.21.0\"\n",
       "2019-04-16 19:34:48  97.105.19.58  \"GET /api/v1/sales?page=84 HTTP/1.1\"     200  511963         NaN  \"python-requests/2.21.0\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '\"https://python.zach.lol/api/V1/HiZach!\"',\n",
       "       '\"https://python.zach.lol/api/v1/stores?page=0\"',\n",
       "       '\"https://python.zach.lol/api/v1/stores?page=1\"',\n",
       "       '\"https://python.zach.lol/api/v1/stores?page=2\"',\n",
       "       '\"https://python.zach.lol/api/v1/stores?page=999\"',\n",
       "       '\"https://python.zach.lol/api/v1/items?page=0\"',\n",
       "       '\"http://localhost:8889/notebooks/timeseries_acquisition.ipynb\"',\n",
       "       '\"https://python.zach.lol/api/v1//api/v1/items?page=2\"',\n",
       "       '\"https://python.zach.lol/api/v1//api/v1/items\"',\n",
       "       '\"https://python.zach.lol/api/v1//api/v1/items/next_page\"',\n",
       "       '\"https://python.zach.lol/api/v1/helloclass!\"',\n",
       "       '\"https://python.zach.lol/api/v1/I_DIDNT_DO_IT!!!!\"',\n",
       "       '\"http://localhost:8888/notebooks/acquire.ipynb\"',\n",
       "       '\"https://python.zach.lol/api/v1/sales?page=3\"',\n",
       "       '\"https://ds.codeup.com/8.3_Acquire/\"',\n",
       "       '\"https://python.zach.lol/\"',\n",
       "       '\"https://python.zach.lol/api/v1/items\"',\n",
       "       '\"https://python.zach.lol/api/v1/\"'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['destination'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.request_method.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two steps below help to \"normalize\" the data by removing a bunch of distinct but unnecessary items, like the page numbers etc.  This will reduce the MANY distinct items down to a handful of distinct items.  Makes this easier, and REDUCES THE NOISE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['request_method', 'request_agent', 'destination']:\n",
    "    df[col] = df[col].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['request_method'] = df.request_method.str.replace(r'\\?page=[0-9]+', '', regex=True)\n",
    "# this finds the \"question mark\\page=number\" and this finds the literal word \"page\", and then find the equal sign,\n",
    "# then find a numerical value, and it's repeated a number of times, hence the plus sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>request_method</th>\n",
       "      <th>status</th>\n",
       "      <th>size</th>\n",
       "      <th>destination</th>\n",
       "      <th>request_agent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:42</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>512495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:42</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/items HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>3561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:44</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>510103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:46</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>510003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:48</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>511963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:48</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/stores HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>1328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:50</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>510753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:52</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>510348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:52</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET / HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:53</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/items HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>3561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ip               request_method  status    size destination           request_agent\n",
       "timestamp                                                                                                         \n",
       "2019-04-16 19:34:42  97.105.19.58   GET /api/v1/sales HTTP/1.1     200  512495         NaN  python-requests/2.21.0\n",
       "2019-04-16 19:34:42  97.105.19.58   GET /api/v1/items HTTP/1.1     200    3561         NaN  python-requests/2.21.0\n",
       "2019-04-16 19:34:44  97.105.19.58   GET /api/v1/sales HTTP/1.1     200  510103         NaN  python-requests/2.21.0\n",
       "2019-04-16 19:34:46  97.105.19.58   GET /api/v1/sales HTTP/1.1     200  510003         NaN  python-requests/2.21.0\n",
       "2019-04-16 19:34:48  97.105.19.58   GET /api/v1/sales HTTP/1.1     200  511963         NaN  python-requests/2.21.0\n",
       "2019-04-16 19:34:48  97.105.19.58  GET /api/v1/stores HTTP/1.1     200    1328         NaN  python-requests/2.21.0\n",
       "2019-04-16 19:34:50  97.105.19.58   GET /api/v1/sales HTTP/1.1     200  510753         NaN  python-requests/2.21.0\n",
       "2019-04-16 19:34:52  97.105.19.58   GET /api/v1/sales HTTP/1.1     200  510348         NaN  python-requests/2.21.0\n",
       "2019-04-16 19:34:52  97.105.19.58               GET / HTTP/1.1     200      42         NaN  python-requests/2.21.0\n",
       "2019-04-16 19:34:53  97.105.19.58   GET /api/v1/items HTTP/1.1     200    3561         NaN  python-requests/2.21.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.request_method.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add variable: converting bytes to mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>size</th>\n",
       "      <th>size_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13,978.00</td>\n",
       "      <td>13,978.00</td>\n",
       "      <td>13,978.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>200.36</td>\n",
       "      <td>450,001.91</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.18</td>\n",
       "      <td>161,491.47</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>200.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>200.00</td>\n",
       "      <td>500,637.00</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>200.00</td>\n",
       "      <td>510,138.00</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>200.00</td>\n",
       "      <td>511,291.00</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>499.00</td>\n",
       "      <td>2,056,327.00</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    status                 size              size_mb\n",
       "count            13,978.00            13,978.00            13,978.00\n",
       "mean                200.36           450,001.91                 0.43\n",
       "std                  10.18           161,491.47                 0.15\n",
       "min                 200.00                 0.00                 0.00\n",
       "25%                 200.00           500,637.00                 0.48\n",
       "50%                 200.00           510,138.00                 0.49\n",
       "75%                 200.00           511,291.00                 0.49\n",
       "max                 499.00         2,056,327.00                 1.96"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['size_mb'] = [n/1024/1024 for n in df['size']]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GET /api/v1/sales HTTP/1.1                      12403\n",
       "GET /api/v1/items HTTP/1.1                       1065\n",
       "GET /api/v1/stores HTTP/1.1                       229\n",
       "GET / HTTP/1.1                                    107\n",
       "GET /documentation HTTP/1.1                       100\n",
       "GET /favicon.ico HTTP/1.1                          26\n",
       "GET /api/v1//api/v1/items HTTP/1.1                 11\n",
       "GET /api/v1/items/api/v1/items HTTP/1.1             7\n",
       "GET /api/v1/items/next_page HTTP/1.1                5\n",
       "GET /api/v1/ HTTP/1.1                               4\n",
       "GET /api/v1/sales/HTTP/1.1                          3\n",
       "GET /api/v1/sales/ HTTP/1.1                         3\n",
       "GET /api/v1/store HTTP/1.1                          3\n",
       "GET /api/v1/itemsitems HTTP/1.1                     3\n",
       "GET /api/v1items HTTP/1.1                           2\n",
       "GET /api/v1/items&page=0 HTTP/1.1                   1\n",
       "GET /api/V1/HiZach! HTTP/1.1                        1\n",
       "GET /api/v1/I_DIDNT_DO_IT!!!! HTTP/1.1              1\n",
       "GET /api/v1/helloclass! HTTP/1.1                    1\n",
       "GET /api/v1/items/HTTP/1.1                          1\n",
       "GET /api/v1 HTTP/1.1                                1\n",
       "GET /api/v1//api/v1/items/next_page HTTP/1.1        1\n",
       "Name: request_method, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.request_method.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GET /api/v1/sales HTTP/1.1                                     0.89\n",
       "GET /api/v1/items HTTP/1.1                                     0.08\n",
       "GET /api/v1/stores HTTP/1.1                                    0.02\n",
       "GET / HTTP/1.1                                                 0.01\n",
       "GET /documentation HTTP/1.1                                    0.01\n",
       "GET /favicon.ico HTTP/1.1                                      0.00\n",
       "GET /api/v1//api/v1/items HTTP/1.1                             0.00\n",
       "GET /api/v1/items/api/v1/items HTTP/1.1                        0.00\n",
       "GET /api/v1/items/next_page HTTP/1.1                           0.00\n",
       "GET /api/v1/ HTTP/1.1                                          0.00\n",
       "GET /api/v1/sales/HTTP/1.1                                     0.00\n",
       "GET /api/v1/sales/ HTTP/1.1                                    0.00\n",
       "GET /api/v1/store HTTP/1.1                                     0.00\n",
       "GET /api/v1/itemsitems HTTP/1.1                                0.00\n",
       "GET /api/v1items HTTP/1.1                                      0.00\n",
       "GET /api/v1/items&page=0 HTTP/1.1                              0.00\n",
       "GET /api/V1/HiZach! HTTP/1.1                                   0.00\n",
       "GET /api/v1/I_DIDNT_DO_IT!!!! HTTP/1.1                         0.00\n",
       "GET /api/v1/helloclass! HTTP/1.1                               0.00\n",
       "GET /api/v1/items/HTTP/1.1                                     0.00\n",
       "GET /api/v1 HTTP/1.1                                           0.00\n",
       "GET /api/v1//api/v1/items/next_page HTTP/1.1                   0.00\n",
       "Name: request_method, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.request_method.value_counts(dropna=False)/df.request_method.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "python-requests/2.21.0                                                                                                                      0.86\n",
       "python-requests/2.20.1                                                                                                                      0.14\n",
       "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36                   0.00\n",
       "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0                                                          0.00\n",
       "Slackbot-LinkExpanding 1.0 (+https://api.slack.com/robots)                                                                                  0.00\n",
       "Slackbot 1.0 (+https://api.slack.com/robots)                                                                                                0.00\n",
       "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36                   0.00\n",
       "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36                   0.00\n",
       "Python-urllib/3.7                                                                                                                           0.00\n",
       "Name: request_agent, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.request_agent.value_counts(dropna=False)/df.request_agent.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Anomalies in Discrete Variables\n",
    "#### Finding anomalies in already existing data\n",
    "We can see easily some anomalies around various request_agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df = pd.DataFrame(df.request_agent.value_counts(dropna=False)).reset_index().\\\n",
    "                       rename(index=str, columns={'index': 'request_agent','request_agent':'agent_count'})\n",
    "\n",
    "agent_df2 = pd.DataFrame(df.request_agent.value_counts(dropna=False)/df.request_agent.count()).reset_index().\\\n",
    "                       rename(index=str, columns={'index': 'request_agent','request_agent':'agent_proba'})\n",
    "agent_df = agent_df.merge(agent_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_agent</th>\n",
       "      <th>agent_count</th>\n",
       "      <th>agent_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>12005</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>python-requests/2.20.1</td>\n",
       "      <td>1911</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slackbot-LinkExpanding 1.0 (+https://api.slack...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Slackbot 1.0 (+https://api.slack.com/robots)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Python-urllib/3.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       request_agent  agent_count          agent_proba\n",
       "0                             python-requests/2.21.0        12005                 0.86\n",
       "1                             python-requests/2.20.1         1911                 0.14\n",
       "2  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4...           34                 0.00\n",
       "3  Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; ...            8                 0.00\n",
       "4  Slackbot-LinkExpanding 1.0 (+https://api.slack...            7                 0.00\n",
       "5       Slackbot 1.0 (+https://api.slack.com/robots)            6                 0.00\n",
       "6  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...            4                 0.00\n",
       "7  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...            2                 0.00\n",
       "8                                  Python-urllib/3.7            1                 0.00"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_agent</th>\n",
       "      <th>agent_count</th>\n",
       "      <th>agent_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slackbot-LinkExpanding 1.0 (+https://api.slack...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Slackbot 1.0 (+https://api.slack.com/robots)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Python-urllib/3.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       request_agent  agent_count          agent_proba\n",
       "2  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4...           34                 0.00\n",
       "3  Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; ...            8                 0.00\n",
       "4  Slackbot-LinkExpanding 1.0 (+https://api.slack...            7                 0.00\n",
       "5       Slackbot 1.0 (+https://api.slack.com/robots)            6                 0.00\n",
       "6  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...            4                 0.00\n",
       "7  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...            2                 0.00\n",
       "8                                  Python-urllib/3.7            1                 0.00"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see those where rate < 1% \n",
    "agent_df[agent_df.agent_proba < .01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df:\n",
      "9\n",
      "different distinct request_agents pinging the Zach_lol server.\n",
      "\n",
      "\n",
      "Tail of df:\n",
      "... the probabilities of those request_agents pinging that server:\n",
      "\n",
      "\n",
      "                                       request_agent  agent_count          agent_proba\n",
      "0                             python-requests/2.21.0        12005                 0.86\n",
      "1                             python-requests/2.20.1         1911                 0.14\n",
      "2  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4...           34                 0.00\n",
      "3  Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; ...            8                 0.00\n",
      "4  Slackbot-LinkExpanding 1.0 (+https://api.slack...            7                 0.00\n",
      "5       Slackbot 1.0 (+https://api.slack.com/robots)            6                 0.00\n",
      "6  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...            4                 0.00\n",
      "7  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3...            2                 0.00\n",
      "8                                  Python-urllib/3.7            1                 0.00\n",
      "\n",
      "\n",
      "Combining the first two request_agent probabilities, this means greater than 99.55% of the probability of hitting this server falls into the two types of python requests, 2.21 and 2.20.\n",
      "\n",
      "\n",
      "\n",
      "This means, most anything hitting this server that is NOT a python 2.20 and 2.21 request, is likely an anomaly or an outlier, for good or bad.\n"
     ]
    }
   ],
   "source": [
    "print('Length of df:')\n",
    "print(len(agent_df))\n",
    "print('different distinct request_agents pinging the Zach_lol server.')\n",
    "print('\\n')\n",
    "print('Tail of df:')\n",
    "print('... the probabilities of those request_agents pinging that server:')\n",
    "print('\\n')\n",
    "print(agent_df.tail(10))\n",
    "print('\\n')\n",
    "print('Combining the first two request_agent probabilities, this means greater than 99.55% of the probability of hitting this server falls into the two types of python requests, 2.21 and 2.20.\\n')\n",
    "print('\\n')\n",
    "print('This means, most anything hitting this server that is NOT a python 2.20 and 2.21 request, is likely an anomaly or an outlier, for good or bad.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the request_agent numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8beef9ad57cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'request_agent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'agent_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     splot.annotate(format(p.get_height(), '.0f'), \n\u001b[1;32m      5\u001b[0m                    \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "splot = sns.barplot(data=agent_df, x = 'request_agent', y = 'agent_count', ci = None)\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), '.0f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', xytext = (0, 10), \n",
    "                   textcoords = 'offset points'\n",
    "                   )\n",
    "    plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting anomalies by establishing a baseline and evaluate as new data arrives\n",
    "#### Establish baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df['2019-04-16 19:34:42':'2019-04-17 12:55:14'][['ip','request_method','status','size','destination','request_agent','size_mb']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute probabilities based on train sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df = pd.DataFrame(train.request_agent.value_counts(dropna=False)/train.request_agent.count()).reset_index().\\\n",
    "                rename(index=str, columns={'index': 'request_agent', 'request_agent': 'agent_proba'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge probabilities with all data (train + new data)\n",
    "#### Where the ip address is new, i.e. not seen in the training dataset, fill the probability with a value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().merge(agent_df, on=['request_agent'], how='left').fillna(value=0).set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agent_proba.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probabilities: probabilities using 2 discrete variables\n",
    "#### Probability of destination given request_agent:\n",
    "If we are looking for an unexpected destination (like weird website, etc) from a known/common request agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('request_agent').size()\n",
    "# this is the count by request_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('request_agent').size().div(len(df))\n",
    "# this is the probability of the distinct request_agent items by the total data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now dividing the size, above, by the length of the df\n",
    "# This is a repeat of the above cell, except setting a new data frame with these items and probs:\n",
    "request_probs = train.groupby('request_agent').size().div(len(df))\n",
    "request_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_given_agent = pd.DataFrame(train.groupby(['request_agent', 'destination']).\\\n",
    "                               size().div(len(train)).\\\n",
    "                               div(request_probs, \n",
    "                                   axis=0, \n",
    "                                   level='request_agent').\\\n",
    "                               reset_index().\\\n",
    "                               rename(index=str, \n",
    "                                      columns={0: 'proba_destination_given_agent'})\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_given_agent.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_destination_count = pd.DataFrame(train.groupby(['request_agent', 'destination'])['request_method'].\\\n",
    "                                count().reset_index().\\\n",
    "                                rename(index=str, \n",
    "                                       columns={'request_method': 'agent_destination_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_destination_count.agent_destination_count.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.request_method.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_destination = destination_given_agent.merge(agent_destination_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_destination.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent_destination.proba_destination_given_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add these probabilities to original events to detect anomalous events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().merge(agent_destination, on=['request_agent', 'destination'], how='left').fillna(value=0).set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.proba_destination_given_agent, df.agent_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series + EMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover users who are accessing our curriculum pages way beyond the end of their codeup time.\n",
    "- What would the dataframe look like?\n",
    "- Use time series method for detecting anomalies, like exponential moving average with %b.\n",
    "- See (paper) notes for good details on how to do this, to include graphics and images, as well as the previous lesson, named:\n",
    "- (\"anomaly_detection_lesson_29Apr19_timeseries_anomalies.ipynb\" in same folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Maggie's walkthrough here... it calculates number of distinct users w/page view per day per cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### she calculates the following:\n",
    "over 7 days\n",
    "\n",
    "for each cohort per day:\n",
    "- EMA\n",
    "- upper bound (ub)\n",
    "- lower bound (lb)\n",
    "- actual value\n",
    "- % bound\n",
    "\n",
    "(current - lb) / divided by / (ub - lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing in data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['date', 'page_viewed', 'user_id', 'cohort', 'ip']\n",
    "df_orig = pd.read_csv('/Users/rachelreuter/ds-methodologies/anomaly_detection/anonymized-curriculum-access.txt',\n",
    "    header=None,\n",
    "    index_col=False,\n",
    "    names=colnames,\n",
    "    sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])',\n",
    "    na_values='\"-\"',\n",
    "    usecols=[0,2,3,4,5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below, keep this code to filter on multiple conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monkey = df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monkey = df_monkey[(df_monkey.cohort.isnull()) & (df_monkey.date > '2018-04-01') & (df_monkey.user_id == 366)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monkey.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep this snippet below to spin off a separate copy of the df, then group and aggregate.\n",
    "#### It creates a new dataframe using a copy of the original dataframe, and brings over only two fields (user_id and date).\n",
    "- .copy() is mandatory here, to avoid affecting the original df, because when it says 'df_b = df_a', it means it is connected.\n",
    "\n",
    "#### Then the next step groups on the user id, and then grabs the min and max from the date column, and creates two additional columns for the min and max log in time for each user for each day."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# note, change this field back to code format above to activate it.\n",
    "\n",
    "user_df = df[['user_id','date']].copy()\n",
    "user_df = user_df.groupby('user_id').agg(['min','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After an aggegation, if the column headers end up on different levels:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# note, change this field back to code format above to activate it.\n",
    "\n",
    "# The min data and max date will be on split levels (use \"user_df.head()\") to see this,   \n",
    "# you can't directly manipulate those columns - so you have to \"droplevel\".  \n",
    "# Once you do that, you can access the columns as min and max.\n",
    "\n",
    "user_df.columns = user_df.columns.droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... now back to the actual code in this lesson..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, perform slight manipulations on the df_orig, drop na rows and converting 'cohort' field to integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df_orig.dropna()\n",
    "df_orig.cohort = df_orig.cohort.astype('int')\n",
    "print(df_orig.info())\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing in the Codeup cohort names, for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['cohort_name','cohort']\n",
    "df_cohort = pd.read_csv('cohort_id_name.txt',\n",
    "                       names=colnames, \n",
    "                       skiprows=1)\n",
    "print(df_cohort.info())\n",
    "df_cohort.head(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the Codeup cohort names with the df, into one df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.merge(df_cohort, on='cohort', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at shape, columns, dtypes and description of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining date and time into one field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig['dateandtime'] = df_orig['date'] + ' ' + df_orig['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing that new combined dateandtime field to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.date)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig['dateandtime'] = pd.to_datetime(df_orig['dateandtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_orig['date'] = pd.to_datetime(df_orig['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the dataframe for blanks, nulls, missing, Nan, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_col(df):\n",
    "\t\"\"\"\n",
    "\tWrite or use a previously written function to return the\n",
    "\ttotal missing values and the percent missing values by column.\n",
    "\t\"\"\"\n",
    "\tnull_count = df.isnull().sum()\n",
    "\tnull_percentage = (null_count / df.shape[0]) * 100\n",
    "\tempty_count = pd.Series(((df == ' ') | (df == '')).sum())\n",
    "\tempty_percentage = (empty_count / df.shape[0]) * 100\n",
    "\tnan_count = pd.Series(((df == 'nan') | (df == 'NaN')).sum())\n",
    "\tnan_percentage = (nan_count / df.shape[0]) * 100\n",
    "\treturn pd.DataFrame({'num_missing': null_count, 'missing_percentage': null_percentage,\n",
    "\t                     'num_empty': empty_count, 'empty_percentage': empty_percentage,\n",
    "\t                     'nan_count': nan_count, 'nan_percentage': nan_percentage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_values_col(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peekatdata(df):\n",
    "    print(\"\\n \\n SHAPE:\")\n",
    "    print(df.shape)\n",
    "\n",
    "    print(\"\\n \\n COLS:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    print(\"\\n \\n INFO:\")\n",
    "    print(df.info())\n",
    "\n",
    "    print(\"\\n \\n Missing Values:\")\n",
    "    missing_vals = df.columns[df.isnull().any()]\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    print(\"\\n \\n DESCRIBE:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    print('\\n \\n HEAD:')\n",
    "    print(df.head(5))\n",
    "\n",
    "    print('\\n \\n TAIL:' )\n",
    "    print(df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peekatdata(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling in any Nans with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_orig = df_orig.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_orig[['cohort']] = df_orig[['cohort']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_orig[['cohort']] = df_orig[['cohort']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_orig[['student_id']] = df_orig[['student_id']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig = field_drop(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_orig = df_orig[['date','dateandtime','ip','cohort','student_id','curriculum_topic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframe now appears to be ready to review.  Will check with .describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframe is prepared and ready to proceed with the anomaly detection lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section below is specifically preparing the data so we can look at the EMA and %B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reiterate the lesson objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover users who are accessing our curriculum pages way beyond the end of their codeup time.\n",
    "- What would the dataframe look like?\n",
    "- Use time series method for detecting anomalies, like exponential moving average with %b.\n",
    "- See (paper) notes for good details on how to do this, to include graphics and images, as well as the previous lesson:\n",
    "- (\"anomaly_detection_lesson_29Apr19_timeseries_anomalies.ipynb\" in same folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a new AGGREGATED dataframe grouped by date, cohort and cohort name, and counting instances of user IDs, for that combination of cohort and cohort name ON THAT DATE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!\n",
    "- Note, groupby code below is LIKE A PIVOT TABLE, where the group by fields are the \"rows\" in the pivot, and the field in the list immediately following is the \"values\" section of a pivot table.\n",
    "- The 'nunique()' aggregation immediately following is like the 'sum, count, average' math function in the values part of the pivot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df.groupby(['date','cohort','cohort_name'])['user_id'].\\\n",
    "nunique().\\\n",
    "reset_index().\\\n",
    "rename(index=str, columns={'user_id': 'users_viewed'})\n",
    "\n",
    "\n",
    "# df_agg = df.groupby(['date','cohort_id','cohort_name'])['user_id'].\\\n",
    "#                         nunique().\\\n",
    "#                     reset_index().\\\n",
    "#                     rename(index=str, \n",
    "#                        columns={'user_id': 'users_viewed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### she calculates the following:\n",
    "over 7 days\n",
    "\n",
    "for each cohort per day:\n",
    "- EMA\n",
    "- upper bound (ub)\n",
    "- lower bound (lb)\n",
    "- actual value\n",
    "- % bound\n",
    "\n",
    "(current - lb) / divided by / (ub - lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential Moving Average (EMA) helps reduce the lag induced by the use of the SMA, or Simple Moving Average. It does this by putting more weight on more recent observations, whereas the SMA weights all observations equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, create separate list of Codeup class cohorts to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = list(df_agg.cohort_name.unique())\n",
    "cohorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, set the index to date on the AGGREGATED df, and rename 'users_viewed' field to 'ema', or 'Exponential Moving Average'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the Exponential Moving Average (EMA) helps reduce the lag induced by the use of the SMA, or Simple Moving Average. It does this by putting more weight on more recent observations, whereas the SMA weights all observations equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_agg.set_index('date').\\\n",
    "rename(index=str, columns={'users_viewed':'ema'}).\\\n",
    "drop(columns='cohort')\n",
    "\n",
    "# df = df_agg.set_index('date').\\\n",
    "#             rename(index=str, columns={'users_viewed':'ema'}).\\\n",
    "#             drop(columns='cohort_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create the Bollinger bands, and setting a standard deviation rolling average (EMA, see further below) of 14 days:\n",
    "\n",
    "Bollinger bands are created by multiplying plus or minus three standard deviations.  The upper bounds and lower bounds are created from the standard deviation.  For lag days inside that initial 14-day rolling window, nans will initially be created, but will be filled in later... see code later for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bollinger_bands(df):\n",
    "    ema = df.ewm(span=14, adjust=False).mean()\n",
    "    ema['stdev'] = ema.ema.rolling(14).std()\n",
    "    ema['ub'] = ema.ema + ema.stdev * 3\n",
    "    ema['lb'] = ema.ema - ema.stdev * 3\n",
    "    return ema.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ema = df[df.cohort_name == 'Arches'].ewm(span=14, adjust=False).mean()\n",
    "# ema['stdev'] = ema.ema.rolling(14).std()\n",
    "# ema['ub'] = ema.ema + ema.stdev * 3\n",
    "# ema['lb'] = ema.ema - ema.stdev * 3\n",
    "# ema.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create new 'bb' df by calling that bollinger_bands() function on the original 'df', where the df.cohort_name matches the list of cohort names, created just above this cell, at the start of the \"EMA\" section.\n",
    "\n",
    "#### Now feed that bb df into an empty list called bands... then concat that list into a new df called 'df2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = []\n",
    "for cohort in cohorts:\n",
    "    bb = bollinger_bands(df[df.cohort_name == cohort])\n",
    "    bands.append(bb)\n",
    "\n",
    "df2 = pd.concat(bands)\n",
    "\n",
    "# bands is a list of dfs, one df for each cohort name\n",
    "\n",
    "\n",
    "# bands = []\n",
    "# for cohort in cohorts:\n",
    "#     bb = bollinger_bands(df[df.cohort_name == cohort])\n",
    "#     bands.append(bb)\n",
    "\n",
    "# df2 = pd.concat(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice the stdev, ub and lb fields in df2 immediately above.  Check for null or nan values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now start to deal with those nans by creating yet another df containing the fields 'cohort_name' and 'ema' from df2, by looking at df2, specifically where the stdev field in df2 is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = df2[df2.stdev.isnull()][['cohort_name','ema']]\n",
    "df_missing.head(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now group the new df_missing df on field 'cohort_name', and fill in the missing stdevs with zero, reset the index, and finally, rename the 'ema' field to 'stdev_null'.\n",
    "\n",
    "#### This groups/aggs the stdevs for all items in each cohort name.\n",
    "\n",
    "#### ...And creates a stdev for any cohort/day where the stdev may be missing, due to the lag days involved in using EMA, in this case 14 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = df_missing.groupby('cohort_name').std().fillna(value=0).reset_index().rename(index=str, columns={'ema': 'stdev_null'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing.head(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now merge the df_missing immediately above (containing the 'missing' stdev values for each cohort) with the df2 from further above, containing the Bollinger Bands for each cohort, for each date.\n",
    "\n",
    "#### Merge this into a new df, merging on 'cohort_name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.merge(df_missing, on='cohort_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, use idx as the index to find where in the df the stdev field is null.\n",
    " \n",
    "#### Then after locating those null rows/records in the stdev field, now fill them with the value in the stdev_null field that we just created above.  Then, afterward, drop the stdev_null field, as it's no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df.stdev.isnull()\n",
    "df.loc[idx,'stdev'] = df.loc[idx,'stdev_null']\n",
    "df = df.drop(columns='stdev_null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note, the nans in the stdev field above have been filled in accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, same routine with the upper and lower bound fields.  These will be filled in with the stdev-calculated formulas shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First locate the null values in the ub field and label with idx.  Then use idx to locate and populate both ub and lb nulls accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df.ub.isnull()\n",
    "df.loc[idx,'ub'] = df.loc[idx,'ema'] + df.loc[idx,'stdev']*3\n",
    "df.loc[idx,'lb'] = df.loc[idx,'ema'] - df.loc[idx,'stdev']*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note, the nulls in stdev, ub and lb in df above are all populated... moving on to next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ub and lb values may be equal in a given row if the ema is equal to 1.  See below for example.\n",
    "\n",
    "This is an error and must be fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.ub == df.lb].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.ub == df.lb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is an error and must be fixed.\n",
    "\n",
    "#### First locate those rows, then nudge each up or down accordingly (if ub or lb, respectively) by .01 to correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df.ub == df.lb\n",
    "df.loc[idx,'ub'] = df.loc[idx,'ub'] + .01\n",
    "df.loc[idx,'lb'] = df.loc[idx,'lb'] - .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now verify the fix worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.ub == df.lb].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Issue above has been fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now reset the date field in the df to datetime datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember the AGGREGATED df_agg df from above, that contained groupings by date, cohort and cohort name, and counted instances of user IDs, for that combination of cohort and cohort name ON THAT DATE?\n",
    "\n",
    "#### Let's pull it back up and look at its info... in preparation for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now merge the df from above with the aggregated df_agg immediately above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with aggregated dataset to get the original count of users viewed\n",
    "df = df.merge(df_agg, on=['cohort_name','date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now compute the %b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute %b\n",
    "df['pct_b'] = (df.users_viewed - df.lb)/(df.ub - df.lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['stdev','cohort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now sort all pct_b values \"descending\", looking for outliers above the 1.0 ub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.pct_b>1].sort_values(by='pct_b', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I can do the same with those pct_b values below the 0.0 lb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.pct_b<0].sort_values(by='pct_b', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now plot out how this looks, with a rolling 7-day exponential moving average (EMA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,16))\n",
    "sns.lineplot(df.date, df.ema, hue=df.cohort_name)\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(16,16))\n",
    "# sns.lineplot(df.date, df.ema, hue=df.cohort_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering - DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Anomalies using Density Based Clustering\n",
    "Clustering-Based Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use dbscan to detect anomalies in other products from the customers dataset (see this same directory).\n",
    "- Use dbscan to detect anomalies in number of bedrooms and finished square feet of property for the filtered dataset you used in the clustering project (single unit properties with a logerror) (ie zillow data).  for this one, see further below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See first cell above for the imports, tools, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "#### *Attribute Information:*\n",
    "\n",
    "1.  FRESH: annual spending (m.u.) on fresh products (Continuous); \n",
    "2.\tMILK: annual spending (m.u.) on milk products (Continuous); \n",
    "3.\tGROCERY: annual spending (m.u.)on grocery products (Continuous); \n",
    "4.\tFROZEN: annual spending (m.u.)on frozen products (Continuous);\n",
    "5.\tDETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous);\n",
    "6.\tDELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous);\n",
    "7.\tCHANNEL: customersâ€™ Channel - Horeca (Hotel/Restaurant/CafÃ©) or Retail channel (Nominal);\n",
    "8.\tREGION: customersâ€™ Region â€“ Lisnon, Oporto or Other (Nominal).\n",
    "\n",
    "#### *Descriptive Statistics:*\n",
    "\n",
    "*(Minimum, Maximum, Mean, Std. Deviation)*\n",
    "- FRESH: (\t3, 112151, 12000.30, 12647.329) \n",
    "- MILK:\t(55, 73498, 5796.27, 7380.377) \n",
    "- GROCERY:\t(3, 92780, 7951.28, 9503.163) \n",
    "- FROZEN:\t(25, 60869, 3071.93, 4854.673) \n",
    "- DETERGENTS_PAPER: (3, 40827, 2881.49, 4767.854) \n",
    "- DELICATESSEN: (3, 47943, 1524.87, 2820.106) \n",
    "\n",
    "REGION\tFrequency\n",
    "- Lisbon\t77\n",
    "- Oporto\t47\n",
    "- Other Region\t316\n",
    "- Total\t440 \n",
    "\n",
    "CHANNEL\tFrequency \n",
    "- Horeca\t298\n",
    "- Retail\t142\n",
    "- Total\t440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv and convert to a df\n",
    "df = pd.read_csv('customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape:')\n",
    "print(df.shape)\n",
    "print('\\nData Types:\\n')\n",
    "print(df.dtypes)\n",
    "print('\\nHead or tail view:')\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, select any two numeric variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab only the Grocery, Milk and Fresh columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Grocery','Milk', 'Fresh']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape:')\n",
    "print(df.shape)\n",
    "print('\\nData Types:\\n')\n",
    "print(df.dtypes)\n",
    "print('\\nHead or tail view:')\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to numpy array (np.array) and set datatype to float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = df.values.astype('float32', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data:\n",
    "- Normalize each attribute by scaling it to 0 mean and unit variance.\n",
    "- This helps to keep the inter-relationships between the features intact so that a small change in one feature would reflect in the other.\n",
    "\n",
    "    *How?*\n",
    "\n",
    "    Use a scaler.  In this case, use Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stscaler = StandardScaler().fit(np_array)\n",
    "np_array = stscaler.transform(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a DBSCAN object.\n",
    "\n",
    "#### *...a DBSCAN object that requires a minimum of 15 data points in a neighborhood of radius 0.5 to be considered a core point.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsc = DBSCAN(eps = .75, min_samples = 15).fit(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... this above constucts the DBSCAN object, and then fits it to the standardized np_array created above.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, extract our cluster labels and outliers to plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dbsc.labels_\n",
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... this is a list of zeros and -1's, where 0 = \"in the cluster,\" and -1 = \"outside the cluster,\" for whatever reason.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below adds the labels to the original df, to analyze and explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = labels\n",
    "df.labels.value_counts()\n",
    "\n",
    "# df['labels'] = labels\n",
    "# df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... above, we have fed the\"labels\" array into a new \"labels\" field in the df, and then done a simple count of values.  This means 40 points are outside the cluster.  These may be anomalies.  Requires investigation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, let's investigate those possible anomalies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.labels == -1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, plot a couple fields against one another:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here is Grocery vs Fresh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df.Grocery, df.Fresh, hue=df.labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*And here is Milk vs Fresh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df.Milk, df.Fresh, hue=df.labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And here is that view in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(8, 8))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "# plot the points\n",
    "ax.scatter(df.Fresh, df.Milk, df.Grocery,\n",
    "           c=df.labels, edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "ax.set_xlabel('Fresh')\n",
    "ax.set_ylabel('Milk')\n",
    "ax.set_zlabel('Grocery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving on to the next question in this lesson:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use dbscan to detect anomalies in number of bedrooms and finished square feet of property for the filtered dataset you used in the clustering project (single unit properties with a logerror)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat all the steps above, using the zillow data instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat all the steps above a third time and maybe use the cvi data I brought in today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bring in the zillow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep it like above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the work I was working through, prior to the walk through...\n",
    "\n",
    "#### Note, MANY of the initial steps were removed and replaced with steps above.  The steps below are after all the manipulations to the df were done, and I was attempting to start grouping/aggregating, but having trouble.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Just pick one cohort to do the grouping on, and then do the timeseries on that one grouped cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample to 30 minute intervals taking min of curriculum page visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = df.groupby([df.index.date,'student_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey2 = df['student_id'].resample('1D').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('student_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('curriculum_topic').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df.index.date).agg(['min','max','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df['curriculum_topic'].resample('30T').min()\n",
    "df2 = df['curriculum_topic'].resample('30T').value_counts.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range(\n",
    "    df2.sort_index().index.min(), \n",
    "    df2.sort_index().index.max(),\n",
    "    freq='30min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reindex(idx, fill_value=0).fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is preparing the data so we can look at the avg, SMA, EMA and %B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_train = df2.head(1).index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date_train = '2019-03-15 23:30:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_test = '2019-03-16 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df2[:end_date_train]\n",
    "test = df2[start_date_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train)\n",
    "plt.plot(test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating upper and lower bands, Maggie provided in the slack channel:\n",
    "\n",
    "span = 24\n",
    "ema_long = train.ewm(span=span, adjust=False).mean()\n",
    "midband = ema_long[-1]\n",
    "ub = midband + ema_long[-24:-1].std()*3\n",
    "lb = midband - ema_long[-24:-1].std()*3\n",
    "\n",
    "yhat['moving_avg_forecast'] = midband"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
