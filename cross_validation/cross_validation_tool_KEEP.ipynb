{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "#### 1. Obtain the cars.csv file from the google classroom and read it into python with pandas.\n",
    "#### 2. Create a feature named gt_avg, which should be either 1 or 0. The value should indicate whether or not a given price is greater than the average price for that car's combination of year, make, and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [c.lower() for c in df]\n",
    "# df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_saleprice'] = df.groupby(['year', 'make', 'model']).price.transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gt_avg'] = (df.price > df.avg_saleprice).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Drop the Id, City, and Vin columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['id', 'price','city','vin','avg_saleprice'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Encode the categorical features as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['state','make','model','year']:\n",
    "    le = LabelEncoder().fit(df[col])  # this creates a numerical label encoder for each value in each column.\n",
    "    df[col] = le.transform(df[col]) # this transforms each item and overwrites the original item in each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>mileage</th>\n",
       "      <th>state</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>gt_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>18681</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>27592</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>13650</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>25195</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>22800</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  mileage  state  make  model  gt_avg\n",
       "0    18    18681     28     7    523       0\n",
       "1    18    27592     19     7    525       0\n",
       "2    18    13650     32     7    526       0\n",
       "3    18    25195     22     7    525       0\n",
       "4    18    22800     38     7    523       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns='gt_avg'), df.gt_avg\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>mileage</th>\n",
       "      <th>state</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62564</th>\n",
       "      <td>19</td>\n",
       "      <td>21413</td>\n",
       "      <td>44</td>\n",
       "      <td>15</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193362</th>\n",
       "      <td>7</td>\n",
       "      <td>87936</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  mileage  state  make  model\n",
       "62564     19    21413     44    15    652\n",
       "193362     7    87936     49     5     13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Hand\n",
    "- Now we can further split our data into training and validate data sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This creates an additional \"test\" data set, or \"VALIDATE\" data set, by doing the same split on the training data itself.\n",
    "\n",
    "- this allows us to test this data on different model/methodology... so we can then pick the best one and THEN use THAT ONE on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=.3333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can explore and feature creation on this smaller training data set, and test it on the validate, like we would normally do on train/test sets.  When it looks good, we can then actually test it on the actual test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now do the k-crossvalidation, by breaking it into several smaller chunks, and cross testing of the chunks.\n",
    "- Breaking train into 3 K's, means training on chunk 1-2, and testing on 3, training on chunks 2-3, and testing on 1, and training on 1-3 and testing on 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63235405, 0.63790846, 0.6348531 , 0.64469676])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4)\n",
    "cross_val_score(tree, X_train, y_train, cv=4)  # 'cv' = number of cross validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64599854, 0.65346659, 0.64935425, 0.65715868])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "cross_val_score(tree, X_train, y_train, cv=4)  # 'cv' = number of cross validation folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- throwing \".mean()\" at the end averages the scores spit out by the cross_val_score tool.  See below.  This is how you'd compare models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, it generates the cross_val_score, like in a confusion matrix.  We can score each of the confusion matrix measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6313004436812996"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "cross_val_score(tree, X_train, y_train, cv=4, scoring='precision').mean()  # 'cv' = number of cross validation folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, sklearn gives the accuracy measure from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6514945150948327"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "cross_val_score(tree, X_train, y_train, cv=4, scoring='accuracy').mean()  # 'cv' = number of cross validation folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens in the multi-class scenario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        50\n",
      "  versicolor       0.98      0.94      0.96        50\n",
      "   virginica       0.94      0.98      0.96        50\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydataset import data\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = data('iris')\n",
    "iris.columns = [c.lower().replace('.', '_') for c in iris]\n",
    "\n",
    "X, y = iris.drop(columns = 'species'), iris.species\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "tree.fit(X, y)\n",
    "\n",
    "actual = y\n",
    "predictions = tree.predict(X)\n",
    "\n",
    "print(classification_report(actual, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.98148148, 0.92156863, 0.98039216])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(tree, X, y, scoring='precision_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below is using the X_train and y_train from the cars data above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [3, 4, 5], 'max_features': [None, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters = {\n",
    "    'max_depth' : [3, 4, 5],\n",
    "    'max_features' : [None, 2, 3],\n",
    "}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid = hyperparameters, cv=3)\n",
    "# we can also change the confusion matrix measures here\n",
    "# like precision, recall, accuracy, etc.\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_max_depth', 'param_max_features', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'split2_train_score', 'mean_train_score', 'std_train_score'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = grid.cv_results_\n",
    "\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62966133, 0.61949058, 0.5758558 , 0.63750338, 0.59276719,\n",
       "       0.62430532, 0.64997766, 0.61558214, 0.63024036])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = results['mean_test_score']\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'max_depth': 3, 'max_features': None},\n",
       " {'max_depth': 3, 'max_features': 2},\n",
       " {'max_depth': 3, 'max_features': 3},\n",
       " {'max_depth': 4, 'max_features': None},\n",
       " {'max_depth': 4, 'max_features': 2},\n",
       " {'max_depth': 4, 'max_features': 3},\n",
       " {'max_depth': 5, 'max_features': None},\n",
       " {'max_depth': 5, 'max_features': 2},\n",
       " {'max_depth': 5, 'max_features': 3}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = results['params']\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'max_depth': 3, 'max_features': None, 'score': 0.6296613316382083},\n",
       " {'max_depth': 3, 'max_features': 2, 'score': 0.6194905813565615},\n",
       " {'max_depth': 3, 'max_features': 3, 'score': 0.5758557968870959},\n",
       " {'max_depth': 4, 'max_features': None, 'score': 0.6375033829073492},\n",
       " {'max_depth': 4, 'max_features': 2, 'score': 0.5927671867427795},\n",
       " {'max_depth': 4, 'max_features': 3, 'score': 0.624305323909445},\n",
       " {'max_depth': 5, 'max_features': None, 'score': 0.6499776570770421},\n",
       " {'max_depth': 5, 'max_features': 2, 'score': 0.6155821432842209},\n",
       " {'max_depth': 5, 'max_features': 3, 'score': 0.6302403595007773}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s, p in zip(scores, params):\n",
    "    p['score'] = s\n",
    "    \n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.575856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.592767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.615582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.619491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.624305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.629661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.630240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.637503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.649978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  max_features     score\n",
       "2          3           3.0  0.575856\n",
       "4          4           2.0  0.592767\n",
       "7          5           2.0  0.615582\n",
       "1          3           2.0  0.619491\n",
       "5          4           3.0  0.624305\n",
       "0          3           NaN  0.629661\n",
       "8          5           3.0  0.630240\n",
       "3          4           NaN  0.637503\n",
       "6          5           NaN  0.649978"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(params).sort_values(by='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
