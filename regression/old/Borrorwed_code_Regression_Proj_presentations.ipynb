{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######========================================================================######\n",
    "\n",
    "# Cody:\n",
    "# How to parse into the data frame to drop certain rows:\n",
    "# check with Joseph for more info on this\n",
    "\n",
    "t1_nan_z2016 = nan_z2016.drop(nan_z2016[\n",
    "    (nan_z2016['calculatedfinishedsquarefeet']<= 400)\n",
    "].index)\n",
    "\n",
    "t2_nan_z2016 = t1_nan_z2016.drop(t1_nan_z2016[\n",
    "    (t1_nan_z2016['bedroomcnt']<= 0)\n",
    "].index)\n",
    "\n",
    "t3_nan_z2016 = t2_nan_z2016.drop(t2_nan_z2016[\n",
    "    (t2_nan_z2016['bathroomcnt']<= 0)\n",
    "].index)\n",
    "\n",
    "t4_nan_z2016 = t3_nan_z2016.drop(t3_nan_z2016[\n",
    "    (t3_nan_z2016['lotsizesquarefeet']<=\n",
    "     t3_nan_z2016['calculatedfinishedsquarefeet'])].index)\n",
    "\n",
    "\n",
    "parsed_data = t4_nan_z2016.sort_values(by=['lotsizesquarefeet',\n",
    "    'bedroomcnt',\n",
    "    'bathroomcnt',\n",
    "    'calculatedfinishedsquarefeet'], ascending=True)\n",
    "\n",
    "df = df.drop(df[df['field'] <= 0)].index)\n",
    "# how to parse into a column into a dataframe and count instances of the values in that column\n",
    "parsed_data.groupby('bedroomcnt').count()[['parcelid']]\n",
    "\n",
    "\n",
    "######========================================================================######\n",
    "# Norrick:\n",
    "# get a copy of his initial functins to check the data, nans, counts, etc.\n",
    "# copy his feature selection and imputing\n",
    "\n",
    "def percent_zeros(df):\n",
    "    cols = list(df.columns)\n",
    "    zeros = {}\n",
    "    for col in cols:\n",
    "        zeros[col] = 'Zeros: {:.2f}%'.format((df[col] == 0).sum()/df.shape[0] * 100)\n",
    "        \n",
    "    return zeros\n",
    "\n",
    "def percent_NaNs(df):\n",
    "    cols = list(df.columns)\n",
    "    Nans = {}\n",
    "    for col in cols:\n",
    "        Nans[col] = 'Missing {:0.2f}%'.format(df[col].isnull().sum()/df.shape[0]*100)\n",
    "        \n",
    "    return Nans\n",
    "\n",
    "def replace_values(df,entry,orginal_value,new):\n",
    "    df[entry] = df[entry].replace(orginal_value,new)\n",
    "    return df\n",
    "\n",
    "def drop_nulls_from(df,column):\n",
    "    return df[df[column].notnull()]\n",
    "\n",
    "\n",
    "Let's see if we can capture info on what is driving the error with regression models\n",
    "\n",
    "Feature Selection and Imputing\n",
    "\n",
    "df = drop_nulls_from(df, 'taxamount')\n",
    "df = drop_nulls_from(df, 'taxvaluedollarcnt' )\n",
    "df = replace_values(df,'garagecarcnt',np.nan,0)\n",
    "df = replace_values(df,'poolcnt',np.nan,0)\n",
    "df = replace_values(df,'yearbuilt',np.nan, df['yearbuilt'].mean())\n",
    "df = replace_values(df,'garagecarcnt',np.nan,0)\n",
    "df = replace_values(df,'calculatedfinishedsquarefeet',np.nan,0)\n",
    "df = replace_values(df,'fullbathcnt',np.nan,0)\n",
    "\n",
    "######========================================================================######\n",
    "\n",
    "# me\n",
    "\n",
    "######========================================================================######\n",
    "# Katy:\n",
    "Explore the data more at the offset, like Maggie did, very methodically and applied life-knowledge and industry-knowledge\n",
    "\n",
    "######========================================================================######\n",
    "\n",
    "# Stephen:\n",
    "grouping, 4-line code he hated... looked good    \n",
    ".duplicated (finds dupes)\n",
    "warning flag for dups\n",
    "like his logic on fillna (how=any, ...) copy this\n",
    "df = np.log(df.field)\n",
    "get his logic on dropping rows... i like this.\n",
    "\n",
    "# Jason:\n",
    "\n",
    "# Gary:\n",
    "# dupe logic and one-to-one logic\n",
    "pd.concat(dupes for _, dupes in logerrors.groupby(\"parcelid\") if len(dupes) > 1)\n",
    "\n",
    "#Now remove the duplicates (keeping the last occurance) based on parcel id\n",
    "logerrors.drop_duplicates(subset='parcelid', keep='last', inplace=True)\n",
    "\n",
    "# and merge the two dataframes, checking for duplicates\n",
    "frames = [properties, logerrors]\n",
    "df = pd.merge(properties, logerrors, on='parcelid', how='inner', validate=\"one_to_one\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Joseph:\n",
    "I like his data prep stage and removing this and that... get this logic.\n",
    "I like this also:   df.isnull().sum    ...use this more\n",
    "copy both of these\n",
    "\n",
    "\n",
    "# Orion:\n",
    "\n",
    "\n",
    "######========================================================================######\n",
    "\n",
    "# Michael:\n",
    "functions, many and handy\n",
    "used our logic to get rid of outlier\n",
    "remove outlier:  can also use the logic of percentiles, ie drop above 95% and below 5%\n",
    "\n",
    "def df_print_r_and_p_values(X, y):\n",
    "    r_and_p_values = {col: stats.pearsonr(X[col], y) for col in X.columns}\n",
    "    print(\"PEARSON'S R\")\n",
    "    for k, v in r_and_p_values.items():\n",
    "        col = k\n",
    "        r, p = v\n",
    "        print(f\"{col}:\")\n",
    "        print(\n",
    "            f\"\\tPearson's R is {r:.2f} with a significance p-value of {p: .3}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "def linreg_fit_and_predict(x_train, y_train, x_test, y_test):\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(x_train, y_train)\n",
    "\n",
    "    y_label = y_train.columns[0]\n",
    "    y_intercept = lm.intercept_[0]\n",
    "    m = lm.coef_[0][0]\n",
    "    x_label = x_train.columns[0]\n",
    "    print(f\"Univariate: {y_label} = {y_intercept:.2f} + {m:.3}*{x_label}\")\n",
    "    print()\n",
    "\n",
    "    preds_train = lm.predict(x_train)\n",
    "\n",
    "    # run test data through model\n",
    "    preds_test = lm.predict(x_test)\n",
    "\n",
    "    return lm, preds_train, preds_test\n",
    "\n",
    "\n",
    "def evaluate_model_train(x, y, preds):\n",
    "    y_label = y.columns[0]\n",
    "    x_label = x.columns[0]\n",
    "\n",
    "    print(\"Model Evaluation on TRAIN Data\")\n",
    "    meanse = mean_squared_error(y, preds)\n",
    "    print(f\"\\tMSE: {meanse:.3f}\")\n",
    "\n",
    "    medianae = median_absolute_error(y, preds)\n",
    "    print(f\"\\tMAE: {medianae:.3f}\")\n",
    "\n",
    "    r2 = r2_score(y, preds)\n",
    "    print(\n",
    "        f\"\\t{r2:.2%} of the variance in {y_label} can be explained by {x_label}.\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    print(\"P-VALUE\")\n",
    "    f_vals, p_vals = f_regression(x, y)\n",
    "    print(f\"\\tTrain: {p_vals[0]:.3}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def evaluate_model_test(x, y, preds):\n",
    "    y_label = y.columns[0]\n",
    "    x_label = x.columns[0]\n",
    "\n",
    "    print(\"Model Evaluation on TEST Data\")\n",
    "    meanse = mean_squared_error(y, preds)\n",
    "    print(f\"\\tMSE: {meanse:.3f}\")\n",
    "\n",
    "    medianae = median_absolute_error(y, preds)\n",
    "    print(f\"\\tMAE: {medianae:.3f}\")\n",
    "\n",
    "    r2 = r2_score(y, preds)\n",
    "    print(\n",
    "        f\"\\t{r2:.2%} of the variance in {y_label} can be explained by {x_label}.\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    print(\"P-VALUE\")\n",
    "    f_vals, p_vals = f_regression(x, y)\n",
    "    print(f\"\\tTest: {p_vals[0]:.3}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def plot_residuals(y_test, preds_test):\n",
    "    y_label = y_test.columns[0]\n",
    "    plt.scatter(preds_test, preds_test - y_test, c=\"g\", s=20)\n",
    "    plt.hlines(y=0, xmin=preds_test.min(), xmax=preds_test.max())\n",
    "    plt.title(\"Residual plot\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.xlabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def linreg_model(x_train, y_train, x_test, y_test):\n",
    "    lm, preds_train, preds_test = linreg_fit_and_predict(\n",
    "        x_train, y_train, x_test, y_test\n",
    "    )\n",
    "\n",
    "    evaluate_model_train(x_train, y_train, preds_train)\n",
    "    evaluate_model_test(x_test, y_test, preds_test)\n",
    "\n",
    "    plot_residuals(y_test, preds_test)\n",
    "    \n",
    "\n",
    "def evaluate_multi_model_train(X, y, preds):\n",
    "    y_label = y.columns[0]\n",
    "    X_labels = X.columns\n",
    "\n",
    "    print(\"Model Evaluation on TRAIN Data\")\n",
    "    meanse = mean_squared_error(y, preds)\n",
    "    print(f\"\\tMSE: {meanse:.3f}\")\n",
    "\n",
    "    medianae = median_absolute_error(y, preds)\n",
    "    print(f\"\\tMAE: {medianae:.3f}\")\n",
    "\n",
    "    r2 = r2_score(y, preds)\n",
    "    print(\n",
    "        f\"\\t{r2:.2%} of the variance in {y_label} can be explained by {X_labels}.\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    print(\"P-VALUE\")\n",
    "    f_vals, p_vals = f_regression(X, y)\n",
    "    print(f\"\\tTrain: {p_vals[0]:.3}\")\n",
    "    print()\n",
    "    \n",
    "def evaluate_multi_model_test(X, y, preds):\n",
    "    y_label = y.columns[0]\n",
    "    X_labels = X.columns\n",
    "\n",
    "    print(\"Model Evaluation on TEST Data\")\n",
    "    meanse = mean_squared_error(y, preds)\n",
    "    print(f\"\\tMSE: {meanse:.3f}\")\n",
    "\n",
    "    medianae = median_absolute_error(y, preds)\n",
    "    print(f\"\\tMAE: {medianae:.3f}\")\n",
    "\n",
    "    r2 = r2_score(y, preds)\n",
    "    print(\n",
    "        f\"\\t{r2:.2%} of the variance in {y_label} can be explained by {X_labels}.\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    print(\"P-VALUE\")\n",
    "    f_vals, p_vals = f_regression(X, y)\n",
    "    print(f\"\\tTest: {p_vals[0]:.3}\")\n",
    "    print()\n",
    "    \n",
    "def multi_linreg_fit_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    y_label = y_train.columns[0]\n",
    "    y_intercept = lm.intercept_[0]\n",
    "    print(\"Multivariate:\")\n",
    "    print(f\"{y_label} = \")\n",
    "    print(f\"{y_intercept:.3f}\")\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        coefficient = lm.coef_[0][i]\n",
    "        print(f\"+ {coefficient:.3}*{col}\")\n",
    "\n",
    "    preds_train = lm.predict(X_train)\n",
    "    evaluate_multi_model_train(X_train, y_train, preds_train)\n",
    "    \n",
    "    preds_test = lm.predict(X_test)\n",
    "    evaluate_model_test(X_test, y_test, preds_test)\n",
    "    \n",
    "    plot_residuals(y_test, preds_test)\n",
    "\n",
    "\n",
    "def normalize_cols(df_train, df_test, cols):\n",
    "    df_train_norm = pd.DataFrame()\n",
    "    for col in cols:\n",
    "        minimum = df_train[col].min()\n",
    "        maximum = df_train[col].max()\n",
    "        df_train_norm[f\"{col}_norm\"] = (df_train[col] - minimum) / (maximum - minimum)\n",
    "    \n",
    "    df_test_norm = pd.DataFrame()\n",
    "    for col in cols:\n",
    "        minimum = df_train[col].min()  # use the min and max from the train set\n",
    "        maximum = df_train[col].max()\n",
    "        df_test_norm[f\"{col}_norm\"] = (df_test[col] - minimum) / (maximum - minimum)\n",
    "    return df_train_norm, df_test_norm    \n",
    "    \n",
    "######========================================================================######    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Nicole:\n",
    "\n",
    "# Matt:\n",
    "Handy:  df.field.valuecounts()\n",
    "plt.subplot logic in cell 56\n",
    "sns.relplot.group by and count year in cell 124\n",
    "\n",
    "# Matthew:\n",
    "cell 32 and 33, binning, grouping, etc.\n",
    "\n",
    "# Eric\n",
    "cell 130 replace nulls\n",
    "\n",
    "\n",
    "\n",
    "def percent_missing(df):\n",
    "\tmissing_table = df.isnull().sum()/df.shape[0]*100\n",
    "\treturn missing_table\n",
    "\n",
    "df['bedbath'] = df['bedroomcnt'] + df['bathroomcnt']\n",
    "df['log_abs'] = df['logerror']\n",
    "df['roombedbath_ratio'] = df['bedbath']/df['roomcnt']\n",
    "df['sqft_per_room'] = df['calculatedfinishedsquarefeet']/df['roomcnt']\n",
    "df['dollar_per_sqft'] = df['taxvaluedollarcnt']/df['calculatedfinishedsquarefeet']\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
